{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d70c564-0a9a-41bf-b276-b4fa4f194c03",
   "metadata": {},
   "source": [
    "Nama : Ahmad Nasrudin Jamil\n",
    "\n",
    "Nim  : 210411100098\n",
    "\n",
    "Kelas : Pencarian dan Penambangan Web B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc93ebe-4dfc-4bec-8aab-fb109673bc37",
   "metadata": {},
   "source": [
    "# Konsep Crawling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34adc71c-bba1-46ab-ab72-2b46526bac21",
   "metadata": {},
   "source": [
    "## Pengertian \n",
    "\n",
    "Crawling adalah proses otomatis yang dilakukan oleh perangkat lunak, yang biasa disebut **web crawler** atau **spider**, untuk menelusuri dan mengumpulkan informasi dari halaman-halaman web di internet. Web crawler secara sistematis mengikuti tautan dari satu halaman ke halaman lainnya, mengindeks konten yang ditemukan, dan menyimpannya dalam basis data atau sistem pengindeksan. Proses ini memungkinkan mesin pencari, seperti Google, untuk membangun indeks besar dari halaman web sehingga pengguna dapat mencari informasi secara efisien.\n",
    "\n",
    "Dalam crawling, web crawler dimulai dari satu atau lebih URL yang disebut **seed**, kemudian mengakses halaman tersebut, dan memproses informasi yang tersedia, termasuk tautan ke halaman lain. Crawler kemudian mengikuti tautan tersebut untuk menemukan halaman web lainnya. Informasi yang diambil oleh crawler biasanya mencakup teks, metadata, dan struktur halaman web.\n",
    "\n",
    "Crawler diatur dengan kebijakan tertentu, seperti **robots.txt** yang ditetapkan oleh pemilik situs web untuk membatasi bagian situs mana yang boleh atau tidak boleh diakses oleh crawler. Tujuan dari crawling adalah untuk memastikan bahwa mesin pencari atau aplikasi lain yang memerlukan data web memiliki akses ke konten yang relevan dan up-to-date.\n",
    "\n",
    "## Tujuan\n",
    "\n",
    "Tujuan dari crawling adalah untuk mengumpulkan, mengindeks, dan memperbarui informasi dari halaman-halaman web di internet. Secara lebih rinci, tujuan-tujuan crawling meliputi:\n",
    "\n",
    "1. **Membangun Indeks Mesin Pencari:** Web crawler digunakan oleh mesin pencari seperti Google untuk mengumpulkan data dari halaman web. Informasi ini kemudian diindeks, sehingga ketika pengguna melakukan pencarian, mesin pencari dapat menemukan dan menampilkan hasil yang relevan dengan cepat.\n",
    "\n",
    "2. **Memastikan Data Terkini:** Crawling memungkinkan mesin pencari untuk terus memperbarui indeksnya dengan data terbaru dari web. Ini penting untuk memberikan hasil pencarian yang akurat dan up-to-date kepada pengguna.\n",
    "\n",
    "3. **Menemukan Halaman Web Baru:** Crawling membantu mesin pencari menemukan halaman web baru yang belum terindeks sebelumnya, sehingga lebih banyak konten yang dapat diakses oleh pengguna melalui pencarian.\n",
    "\n",
    "4. **Mengumpulkan Data untuk Analisis:** Perusahaan atau organisasi dapat menggunakan crawler untuk mengumpulkan data dari web untuk berbagai tujuan, seperti analisis pasar, pemantauan kompetitor, atau penelitian.\n",
    "\n",
    "5. **Mengoptimalkan Pengalaman Pengguna:** Dengan memastikan bahwa konten yang diindeks relevan dan berkualitas, crawling membantu mesin pencari memberikan pengalaman pencarian yang lebih baik bagi pengguna.\n",
    "\n",
    "6. **Pemantauan Situs Web:** Crawling juga digunakan untuk memantau situs web, seperti untuk mengidentifikasi masalah teknis, memastikan kepatuhan terhadap standar SEO, atau melacak perubahan konten.\n",
    "\n",
    "\n",
    "## Jenis-Jenis \n",
    "\n",
    "1. **News Crawling** adalah proses pengumpulan data dari situs berita dan portal berita online. Tujuannya adalah untuk memastikan bahwa mesin pencari memiliki informasi terkini dan relevan dari berbagai sumber berita sehingga pengguna dapat menemukan berita terbaru dengan cepat.\n",
    "\n",
    "2. **Social Media Crawling** adalah proses pengumpulan data dari platform media sosial seperti Twitter, Facebook, dan Instagram. Tujuannya adalah untuk mengumpulkan data yang dapat digunakan untuk analisis tren, pemantauan merek, atau riset pemasaran, serta memahami opini publik atau reaksi terhadap peristiwa tertentu.\n",
    "\n",
    "3. **Video Crawling** adalah proses pengumpulan informasi dari situs-situs berbagi video seperti YouTube dan Vimeo. Tujuannya adalah untuk mengindeks video untuk mesin pencari video atau untuk analisis tren, rekomendasi konten, atau pengelompokan video berdasarkan topik.\n",
    "\n",
    "4. **Image Crawling** adalah proses pengumpulan gambar dari web. Tujuannya adalah untuk mengindeks gambar untuk mesin pencari gambar, melatih model pengenalan gambar, atau untuk riset yang memerlukan pengumpulan data visual.\n",
    "\n",
    "5. **Email Crawling** adalah proses pengumpulan alamat email dari berbagai situs web. Tujuannya adalah untuk mengumpulkan alamat email untuk keperluan pemasaran, membangun daftar pelanggan, atau dalam beberapa kasus, untuk aktivitas spam.\n",
    "   \n",
    "## Contoh Web Crawling\n",
    "\n",
    "1. **Duckduc Bot** adalah bot web yang digunakan oleh mesin pencari DuckDuckGo. Tujuannya adalah untuk mengindeks halaman web sehingga informasi tersebut dapat muncul dalam hasil pencarian DuckDuckGo, yang menonjolkan privasi pengguna sebagai keunggulan utamanya.\n",
    "\n",
    "2. **Baiduspider** adalah bot web yang dimiliki oleh Baidu, mesin pencari terbesar di Tiongkok. Tujuannya adalah untuk mengindeks halaman-halaman web, terutama untuk pasar Tiongkok, agar konten tersebut dapat diakses melalui hasil pencarian Baidu.\n",
    "\n",
    "3. **Alexabot** adalah bot yang digunakan oleh Alexa Internet, anak perusahaan Amazon, untuk mengumpulkan data tentang lalu lintas web. Tujuannya adalah untuk menyediakan analisis, peringkat, dan statistik web yang digunakan oleh layanan Alexa untuk memantau kinerja situs web.\n",
    "\n",
    "4. **Yahoo! Slurp Bot** adalah crawler yang digunakan oleh Yahoo! untuk mengindeks halaman web. Tujuannya adalah memastikan konten yang diindeks dapat ditemukan dan ditampilkan dalam hasil pencarian Yahoo!, yang merupakan salah satu mesin pencari terkemuka di masa lalu.\n",
    "\n",
    "5. **Yandex Bot** adalah bot web yang digunakan oleh Yandex, mesin pencari terbesar di Rusia. Tujuannya adalah untuk mengindeks halaman web agar informasi tersebut tersedia dalam hasil pencarian Yandex, yang melayani pengguna di Rusia dan negara-negara berbahasa Rusia lainnya.\n",
    "\n",
    "6. **Bingbot** adalah bot web milik Microsoft yang digunakan oleh mesin pencari Bing. Tujuannya adalah untuk mengindeks konten dari halaman web di seluruh dunia sehingga informasi tersebut dapat ditemukan dalam hasil pencarian Bing.\n",
    "\n",
    "7. **Facebook External Hit** Hit adalah bot yang digunakan oleh Facebook untuk merayapi halaman web ketika pengguna membagikan tautan di platform tersebut. Tujuannya adalah untuk mengumpulkan metadata dan gambar dari halaman web guna membuat pratinjau (preview) yang menarik saat tautan dibagikan di Facebook.\n",
    "\n",
    "## Cara Kerja \n",
    "\n",
    "Cara kerja crawling melibatkan beberapa langkah utama yang dilakukan oleh program komputer yang disebut crawler atau spider. Berikut adalah penjelasan langkah demi langkah mengenai cara kerja crawling:\n",
    "\n",
    "1. **Inisialisasi URL**: Crawling dimulai dengan daftar URL awal yang disebut sebagai \"seed URLs.\" URL ini biasanya diambil dari halaman-halaman web yang sudah diketahui penting atau populer, seperti halaman beranda situs terkenal.\n",
    "\n",
    "2. **Mengunjungi Halaman Web**: Crawler mengunjungi setiap URL dalam daftar seed. Saat mengunjungi halaman, crawler membaca dan mengunduh konten halaman web tersebut, termasuk teks, gambar, dan elemen-elemen lainnya.\n",
    "\n",
    "3. **Ekstraksi Tautan (Link Extraction)**: Setelah mengunduh halaman, crawler mengekstrak semua tautan (link) yang ada di dalamnya. Tautan-tautan ini menunjuk ke halaman lain baik di dalam situs yang sama maupun di situs web lain. Tautan baru ini ditambahkan ke daftar URL yang harus dikunjungi.\n",
    "\n",
    "4. **Penyimpanan dan Indeksasi**: Crawler menyimpan salinan halaman web yang diunduh dan mengirimkan informasi yang relevan ke mesin indeks. Mesin indeks mengolah informasi ini untuk mempermudah pencarian di masa mendatang, misalnya dengan menganalisis kata kunci, metadata, dan struktur halaman.\n",
    "\n",
    "5. **Mengikuti Tautan**: Crawler kemudian mengunjungi tautan-tautan baru yang telah diekstrak. Proses ini diulang terus menerus, menciptakan siklus yang memungkinkan crawler untuk menjelajahi seluruh web secara bertahap.\n",
    "\n",
    "6. **Penyaringan (Filtering)**: Tidak semua halaman yang ditemukan oleh crawler diindeks. Crawler menggunakan berbagai aturan untuk menyaring halaman yang dianggap tidak relevan, seperti halaman yang berisi spam, duplikat konten, atau halaman dengan sedikit konten.\n",
    "\n",
    "7. **Pemutakhiran (Update Crawling)**: Crawlers juga secara berkala mengunjungi ulang halaman yang sudah diindeks untuk memperbarui informasi yang ada. Halaman yang sering berubah mungkin lebih sering dikunjungi ulang oleh crawler.\n",
    "\n",
    "8. **Prioritas (Crawling Priority)**: Crawler biasanya memprioritaskan halaman berdasarkan berbagai faktor, seperti popularitas, kualitas tautan, atau seberapa sering konten halaman diperbarui.\n",
    "\n",
    "9. **Kepatuhan terhadap Robot.txt**: Sebelum mengunjungi halaman, crawler memeriksa file `robots.txt` di setiap situs untuk melihat apakah ada aturan yang membatasi bagian mana dari situs yang boleh atau tidak boleh diakses.\n",
    "\n",
    "10. **Penanganan Kesalahan**: Crawler juga menangani berbagai kesalahan yang mungkin terjadi, seperti halaman tidak ditemukan (404 error), waktu tunggu (timeout), atau masalah akses lainnya.\n",
    "\n",
    "Dengan mengikuti langkah-langkah ini, crawler dapat menjelajahi web secara efisien, mengumpulkan data yang dibutuhkan, dan memastikan mesin pencari memiliki indeks yang up-to-date dan relevan untuk digunakan dalam menampilkan hasil pencarian kepada pengguna.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8f31a-06f3-46d5-a946-4ccf9c87f22c",
   "metadata": {},
   "source": [
    "# Teknik dan Cara Crawling Menggunakan Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab1f003-5674-481f-8544-be476effad70",
   "metadata": {},
   "source": [
    "Crawling menggunakan Python melibatkan beberapa teknik yang memungkinkan pengumpulan data dari situs web secara otomatis. Teknik dasar termasuk penggunaan pustaka `requests` untuk mengunduh halaman web, di mana kontennya dapat diambil dan diproses. Setelah halaman diunduh, pustaka `BeautifulSoup` sering digunakan untuk memparsing dokumen HTML atau XML, memungkinkan ekstraksi elemen spesifik seperti tautan atau teks. Untuk tugas crawling yang lebih kompleks dan terstruktur, `Scrapy` adalah framework populer yang menawarkan kemampuan untuk membangun spider yang dapat menjelajahi dan mengumpulkan data dari banyak halaman web secara efisien. Untuk situs web dinamis yang mengandalkan JavaScript, `Selenium` digunakan untuk mengotomatisasi browser dan mengambil konten yang di-render, memastikan data dari situs dinamis dapat diakses. Selain itu, teknik tambahan seperti menambahkan `User-Agent` dan jeda antar permintaan dengan `time.sleep` digunakan untuk menghindari pemblokiran oleh situs web, menjaga agar proses crawling terlihat lebih alami dan menghindari deteksi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07eb0ae-debc-4fe0-907a-55a9afdda800",
   "metadata": {},
   "source": [
    "# Tool dan Library Yang Digunakan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e30ae-aaf8-4bd8-b984-6dcbc4f9ec4f",
   "metadata": {},
   "source": [
    "Sebgai contoh disini menggunakan Beautiful Soup untuk crawling website berita, maka harus mempersiapkan beberapa tools dan library. Berikut langkah-langkahnya:\n",
    "\n",
    "1. **Install Python**:\n",
    "   Memastikan Python terinstal di sistem, jika belum maka dapat mengunduhnya dari [python.org](https://www.python.org/downloads/).\n",
    "\n",
    "2. **Install Library yang Dibutuhkan**:\n",
    "   - **Requests**: Untuk mengirim permintaan HTTP ke situs web yang ingin dicrawl.\n",
    "   - **Beautiful Soup**: Untuk memparsing HTML dan mengekstrak data yang diinginkan.\n",
    "\n",
    "   Menginstal kedua library ini menggunakan pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install requests\n",
    "   pip install beautifulsoup4\n",
    "   ```\n",
    "\n",
    "3. **Library Tambahan (Opsional)**:\n",
    "   **lxml** atau **html5lib**: Untuk parsing HTML yang lebih kuat, maka dapat menginstal parser ini. Beautiful Soup mendukung berbagai parser, tetapi `lxml` atau `html5lib` sering digunakan untuk parsing yang lebih kompleks.\n",
    "\n",
    "   ```bash\n",
    "   pip install lxml\n",
    "   pip install html5lib\n",
    "   ```\n",
    "\n",
    "4. **IDE atau Text Editor**:\n",
    "   Bisa juga menggunakan IDE atau text editor seperti PyCharm, VSCode, atau bahkan Notepad++ untuk menulis skrip Python.\n",
    "\n",
    "### Contoh Penggunaan\n",
    "\n",
    "Berikut adalah contoh sederhana menggunakan Requests dan Beautiful Soup untuk melakukan crawling:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL yang ingin di crawl\n",
    "url = 'https://example.com'\n",
    "\n",
    "# Mengirimkan permintaan HTTP ke URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Memastikan permintaan berhasil\n",
    "if response.status_code == 200:\n",
    "    # Parsing HTML menggunakan Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Mengekstrak data yang diinginkan, misalnya semua judul artikel\n",
    "    for title in soup.find_all('h1'):\n",
    "        print(title.text)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage.\")\n",
    "```\n",
    "\n",
    "### Tips Penting:\n",
    "- **Patuhilah Robots.txt**: Sebelum melakukan crawling, pastikan untuk memeriksa file `robots.txt` situs web tersebut untuk melihat aturan yang harus dipatuhi.\n",
    "- **Rate Limiting**: Untuk menghindari pembatasan atau pemblokiran, tambahkan jeda waktu antara permintaan.\n",
    "- **Menggunakan User-Agent**: Kadang-kadang perlu untuk menyetel `User-Agent` header dalam permintaan HTTP untuk menghindari deteksi sebagai bot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884ac11-f114-4ac6-96f4-ae77fb403129",
   "metadata": {},
   "source": [
    "# Proses Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2778c214-9b17-4694-b951-14e7d3940cf7",
   "metadata": {},
   "source": [
    "### Library yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffb1a659-4091-422b-b045-cab1356782a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb9dd3-16c8-487a-98cb-cace89721e93",
   "metadata": {},
   "source": [
    "### Code Implementasi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a306cad-bc53-449a-be93-5549fd3344e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store results\n",
    "article_result = []\n",
    "\n",
    "# Iterasi page dari 1 to 10\n",
    "for page_num in range(1, 11):\n",
    "    url = \"https://indeks.kompas.com/?page={}\".format(page_num)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='articleItem')\n",
    "    \n",
    "    for article in articles:\n",
    "        article_url = article.find('a', class_='article-link')['href']\n",
    "        title = article.find('h2', class_='articleTitle').text.strip()\n",
    "        category = article.find('div', class_='articlePost-subtitle').text.strip()\n",
    "        date = article.find('div', class_='articlePost-date').text.strip()\n",
    "\n",
    "        cPage = requests.get(article_url)\n",
    "        cSoup = BeautifulSoup(cPage.text, 'html.parser')\n",
    "\n",
    "        content_paragraphs = cSoup.find('div', class_='read__content').find_all('p')\n",
    "        content = \"\\n\".join([p.text.strip() for p in content_paragraphs])\n",
    "        \n",
    "        # Append the article details to the results list\n",
    "        article_result.append([title, content, date, category])\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "df = pd.DataFrame(article_result, columns=['title', 'content', 'date', 'category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc574de6-925d-4bfc-9008-11a137c9bdc7",
   "metadata": {},
   "source": [
    "### Penjelasan Code\n",
    "\n",
    "Berikut ini adalah penjelasan untuk kode diatas:\n",
    "\n",
    " 1. Inisialisasi List Kosong untuk Menyimpan Hasil\n",
    "    ```python\n",
    "    article_result = []\n",
    "    ```\n",
    "    `article_result` adalah list kosong yang akan digunakan untuk menyimpan data dari setiap artikel yang berhasil diambil dari beberapa halaman web.\n",
    "\n",
    " 2. Iterasi Halaman dari 1 hingga 10\n",
    "    ```python\n",
    "    for page_num in range(1, 11):\n",
    "        url = \"https://indeks.kompas.com/?page={}\".format(page_num)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "        articles = soup.find_all('div', class_='articleItem')\n",
    "    ```\n",
    "    \n",
    "    - **`for page_num in range(1, 11)`**: Loop ini digunakan untuk mengiterasi halaman web dari 1 hingga 10. `range(1, 11)` menghasilkan angka dari 1 sampai 10 (inklusif).\n",
    "    - **`url = \"https://indeks.kompas.com/?page={}\".format(page_num)`**: Untuk setiap nomor halaman (`page_num`), kode ini membangun URL dengan menambahkan nomor halaman sebagai parameter di akhir URL.\n",
    "    - **`page = requests.get(url)`**: Mengirimkan permintaan HTTP GET ke URL yang dibentuk untuk setiap halaman, dan respons HTML disimpan di variabel `page`.\n",
    "    - **`soup = BeautifulSoup(page.text, 'html.parser')`**: Mem-parsing konten HTML dari `page` menggunakan BeautifulSoup untuk diolah lebih lanjut.\n",
    "    - **`articles = soup.find_all('div', class_='articleItem')`**: Mencari semua elemen `div` dengan class `articleItem` dalam halaman web tersebut. Setiap elemen `div` ini dianggap mewakili satu artikel di halaman tersebut.\n",
    "\n",
    "3. Mengiterasi Artikel dalam Setiap Halaman\n",
    "    ```python\n",
    "    for article in articles:\n",
    "        article_url = article.find('a', class_='article-link')['href']\n",
    "        title = article.find('h2', class_='articleTitle').text.strip()\n",
    "        category = article.find('div', class_='articlePost-subtitle').text.strip()\n",
    "        date = article.find('div', class_='articlePost-date').text.strip()\n",
    "    ```\n",
    "    \n",
    "    - **`for article in articles`**: Loop ini mengiterasi setiap artikel yang ditemukan dalam halaman yang sedang diproses.\n",
    "    - **`article_url = article.find('a', class_='article-link')['href']`**: Mengambil URL lengkap dari artikel dengan mencari elemen `a` yang memiliki class `article-link` dan mengambil atribut `href`.\n",
    "    - **`title = article.find('h2', class_='articleTitle').text.strip()`**: Mengambil judul artikel dari elemen `h2` dengan class `articleTitle` dan menghapus spasi yang tidak perlu menggunakan `strip()`.\n",
    "    - **`category = article.find('div', class_='articlePost-subtitle').text.strip()`**: Mengambil kategori artikel dari elemen `div` dengan class `articlePost-subtitle` dan membersihkannya dari spasi yang tidak perlu.\n",
    "    - **`date = article.find('div', class_='articlePost-date').text.strip()`**: Mengambil tanggal artikel dari elemen `div` dengan class `articlePost-date`.\n",
    "\n",
    "4. Mengambil Konten dari Setiap Artikel\n",
    "    ```python\n",
    "    cPage = requests.get(article_url)\n",
    "    cSoup = BeautifulSoup(cPage.text, 'html.parser')\n",
    "    \n",
    "    content_paragraphs = cSoup.find('div', class_='read__content').find_all('p')\n",
    "    content = \"\\n\".join([p.text.strip() for p in content_paragraphs])\n",
    "    ```\n",
    "    \n",
    "    - **`cPage = requests.get(article_url)`**: Mengirimkan permintaan HTTP GET ke URL artikel yang diambil pada langkah sebelumnya.\n",
    "    - **`cSoup = BeautifulSoup(cPage.text, 'html.parser')`**: Mem-parsing halaman artikel untuk mengambil konten utamanya.\n",
    "    - **`content_paragraphs = cSoup.find('div', class_='read__content').find_all('p')`**: Mencari elemen `div` dengan class `read__content` yang biasanya menyimpan isi utama dari artikel, dan mengambil semua paragraf (`<p>`).\n",
    "    - **`content = \"\\n\".join([p.text.strip() for p in content_paragraphs])`**: Menggabungkan teks dari setiap paragraf menjadi satu string besar, dipisahkan oleh newline (`\\n`).\n",
    "\n",
    "5. Menyimpan Data Artikel ke dalam List\n",
    "    ```python\n",
    "    article_result.append([title, content, date, category])\n",
    "    ```\n",
    "    Data artikel yang terdiri dari judul (`title`), konten (`content`), tanggal (`date`), dan kategori (`category`) disimpan sebagai satu entri dalam list `article_result`.\n",
    "\n",
    "6. Mengonversi Hasil Menjadi DataFrame\n",
    "    ```python\n",
    "    df = pd.DataFrame(article_result, columns=['title', 'content', 'date', 'category'])\n",
    "    ```\n",
    "    Setelah semua halaman dan artikel diproses, `article_result` diubah menjadi DataFrame dengan menggunakan `pandas`, yang memudahkan analisis data lebih lanjut. Kolom DataFrame diberi nama `title`, `content`, `date`, dan `category`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c4a32f-e4f2-4e6b-81c6-7cd32ab6800d",
   "metadata": {},
   "source": [
    "# Output Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e19af17-0012-47c5-856c-7dab43b9000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sebulan Pascanikah, Aaliyah Massaid: Sekarang ...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Artis Aaliyah Massaid pa...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>HYPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beasiswa Bill Gates 2025 Dibuka, Kuliah S2-S3 ...</td>\n",
       "      <td>KOMPAS.com - Beasiswa Bill Gates 2025 sudah di...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>EDUKASI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suami yang Bunuh Istrinya di Pasar Minggu Semp...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - A (30), sempat dikeroyok...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Motor Baru Yamaha Masih Harus Inreyen, Jangan ...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Beredar video Yamaha NMA...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>OTOMOTIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Punya Penghasilan RP 1,5 Triliun, Kucing Taylo...</td>\n",
       "      <td>KOMPAS.com- Kucing Taylor Swift, Olivia Benson...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>HYPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Seorang Istri Dibunuh Suaminya di Rumah Kontra...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Seorang wanita berinisia...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Indonesia Lihat Peluang Kerjasama Baterai EV d...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Menteri Investasi/Kepala...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>OTOMOTIF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>9.000 Aparat Gabungan Dikerahkan Amankan Misa ...</td>\n",
       "      <td>JAKARTA, KOMPAS.com - Sebanyak 9.000 aparat ga...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Pantai Ria di Ende, NTT: Daya Tarik, Harga Tik...</td>\n",
       "      <td>KOMPAS.com - Pantai Ria adalah salah satu pant...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>REGIONAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>GOTO Tutup Layanan Gojek di Vietnam</td>\n",
       "      <td>JAKARTA, KOMPAS.com - PT GoTo Gojek Tokopedia ...</td>\n",
       "      <td>04/09/2024</td>\n",
       "      <td>MONEY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Sebulan Pascanikah, Aaliyah Massaid: Sekarang ...   \n",
       "1    Beasiswa Bill Gates 2025 Dibuka, Kuliah S2-S3 ...   \n",
       "2    Suami yang Bunuh Istrinya di Pasar Minggu Semp...   \n",
       "3    Motor Baru Yamaha Masih Harus Inreyen, Jangan ...   \n",
       "4    Punya Penghasilan RP 1,5 Triliun, Kucing Taylo...   \n",
       "..                                                 ...   \n",
       "115  Seorang Istri Dibunuh Suaminya di Rumah Kontra...   \n",
       "116  Indonesia Lihat Peluang Kerjasama Baterai EV d...   \n",
       "117  9.000 Aparat Gabungan Dikerahkan Amankan Misa ...   \n",
       "118  Pantai Ria di Ende, NTT: Daya Tarik, Harga Tik...   \n",
       "119                GOTO Tutup Layanan Gojek di Vietnam   \n",
       "\n",
       "                                               content        date  category  \n",
       "0    JAKARTA, KOMPAS.com - Artis Aaliyah Massaid pa...  04/09/2024      HYPE  \n",
       "1    KOMPAS.com - Beasiswa Bill Gates 2025 sudah di...  04/09/2024   EDUKASI  \n",
       "2    JAKARTA, KOMPAS.com - A (30), sempat dikeroyok...  04/09/2024      NEWS  \n",
       "3    JAKARTA, KOMPAS.com - Beredar video Yamaha NMA...  04/09/2024  OTOMOTIF  \n",
       "4    KOMPAS.com- Kucing Taylor Swift, Olivia Benson...  04/09/2024      HYPE  \n",
       "..                                                 ...         ...       ...  \n",
       "115  JAKARTA, KOMPAS.com - Seorang wanita berinisia...  04/09/2024      NEWS  \n",
       "116  JAKARTA, KOMPAS.com - Menteri Investasi/Kepala...  04/09/2024  OTOMOTIF  \n",
       "117  JAKARTA, KOMPAS.com - Sebanyak 9.000 aparat ga...  04/09/2024      NEWS  \n",
       "118  KOMPAS.com - Pantai Ria adalah salah satu pant...  04/09/2024  REGIONAL  \n",
       "119  JAKARTA, KOMPAS.com - PT GoTo Gojek Tokopedia ...  04/09/2024     MONEY  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71b88b21-9f84-418f-bbd7-468a53769bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data berita telah disimpan ke berita_crawling_kompas.csv\n"
     ]
    }
   ],
   "source": [
    "csv_filename = \"berita_crawling_kompas.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data berita telah disimpan ke {csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
